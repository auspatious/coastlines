{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vietnam Coastlines Combined\n",
    "\n",
    "\n",
    "* Load stack of all available Landsat 5, 7, 8 and 9 satellite imagery for a location \n",
    "* Convert each satellite image into a remote sensing water index (MNDWI)\n",
    "* For each satellite image, model ocean tides into a grid based on exact time of image acquisition\n",
    "* Interpolate tide heights into spatial extent of image stack using the [FES2014 global tide model](https://github.com/GeoscienceAustralia/dea-coastlines/wiki/Setting-up-tidal-models-for-DEA-Coastlines)\n",
    "* Mask out high and low tide pixels by removing all observations acquired outside of 50 percent of the observed tidal range centered over mean sea level\n",
    "* Combine tidally-masked data into annual median composites representing the most representative position of the coastline at approximately mean sea level each year\n",
    "* Apply morphological extraction algorithms to mask annual median composite rasters to a valid coastal region\n",
    "* Extract waterline vectors using subpixel waterline extraction ([Bishop-Taylor et al. 2019b](https://doi.org/10.3390/rs11242984))\n",
    "* Compute rates of coastal change at every 30 m using linear regression\n",
    "\n",
    "This is an interactive version of the code intended for prototyping; to run this analysis at scale, use the [command line tools](DEACoastlines_generation_CLI.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Set working directory to top level of repository to ensure links work correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "\n",
    "First we import the required Python packages, then we connect to the database, and load the catalog of virtual products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "\n",
    "# Load DEA Coastlines and DEA tools code\n",
    "from coastlines.utils import get_study_site_geometry, load_config\n",
    "from coastlines.combined import (\n",
    "    load_and_mask_data_with_stac,\n",
    "    export_results,\n",
    "    filter_by_tides,\n",
    "    generate_yearly_composites,\n",
    "    mask_pixels_by_tide,\n",
    "    sanitise_tile_id,\n",
    ")\n",
    "from coastlines.vector import (\n",
    "    points_on_line,\n",
    "    annual_movements,\n",
    "    calculate_regressions,\n",
    "    all_time_stats,\n",
    "    contours_preprocess,\n",
    "    contour_certainty,\n",
    "    points_certainty,\n",
    ")\n",
    "\n",
    "# Load other libraries\n",
    "from pathlib import Path\n",
    "import geopandas as gpd\n",
    "from datacube.utils.dask import start_local_dask\n",
    "from dea_tools.spatial import subpixel_contours\n",
    "from odc.stac import configure_s3_access"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hide warnings. Don't run this cell to see the warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create local dask client for parallelisation\n",
    "dask_client = start_local_dask(\n",
    "    n_workers=4, threads_per_worker=8, mem_safety_margin=\"2GB\"\n",
    ")\n",
    "\n",
    "# Configure S3 access including request payer\n",
    "_ = configure_s3_access(requester_pays=True, cloud_defaults=True)\n",
    "\n",
    "print(dask_client.dashboard_link.replace(\"/user\", \"https://hub.asia.easi-eo.solutions/user\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Study area selection\n",
    "# study_area = \"13,45\" # North\n",
    "study_area = \"9,19\"  # South west\n",
    "# study_area = \"18,32\" # Central\n",
    "\n",
    "# Issues!\n",
    "\n",
    "# Still outstanding issues:\n",
    "# study_area = \"34,21\"  # Very noisy and more of a reef than an island. No ESA WC data.\n",
    "# study_area = \"33,22\"  # Works, but noisy and some of the area is being clipped off\n",
    "# study_area = \"32,18\"  # Not enough data.\n",
    "\n",
    "# study_area = \"6,20\"  # Noisy ocean, so look at different masking\n",
    "# study_area = \"33,21\"  # Very noisy and nothing is generated.\n",
    "\n",
    "# Atoll with lots of noise\n",
    "# study_area = \"28,15\"\n",
    "# study_area = \"29,15\"  # Failed again.\n",
    "\n",
    "# Atolls...\n",
    "# study_area = \"31,19\"\n",
    "# study_area = \"32,23\"\n",
    "# study_area = \"33,24\"\n",
    "# study_area = \"26,35\"\n",
    "\n",
    "# Important atolls\n",
    "# study_area = \"29,24\"  # Alex is exploring this one\n",
    "# study_area = \"27,21\"\n",
    "# study_area = \"24,19\"\n",
    "# study_area = \"18,22\"\n",
    "\n",
    "# Config\n",
    "version = \"testing\"\n",
    "start_year = 2012\n",
    "end_year = 2022\n",
    "baseline_year = 2021\n",
    "water_index = \"mndwi\"\n",
    "index_threshold = 0.0\n",
    "\n",
    "config_path = \"configs/vietnam_coastlines_config_development.yaml\"\n",
    "\n",
    "# Tide data and config\n",
    "home = Path(\"~\")\n",
    "tide_data_location = f\"{home}/tide_models\"\n",
    "tide_centre = 0.0\n",
    "\n",
    "# Output config\n",
    "output_dir = Path(f\"data/interim/vector/{version}/{sanitise_tile_id(study_area)}_{version}\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "output_cache_zarr = output_dir / f\"{sanitise_tile_id(study_area)}_{version}_combined_ds.zarr\"\n",
    "\n",
    "# Load analysis params from config file\n",
    "config = load_config(config_path=config_path)\n",
    "\n",
    "# Load the geometry from the grid used for the location\n",
    "geometry = get_study_site_geometry(config[\"Input files\"][\"grid_path\"], study_area)\n",
    "\n",
    "# BBOX and other query parameters\n",
    "bbox = list(geometry.buffer(0.05).bounds.values[0])\n",
    "\n",
    "# Use the USGS STAC API to identify scenes to load\n",
    "query = {\n",
    "    \"bbox\": bbox,\n",
    "    \"datetime\": (str(start_year - 1), str(end_year + 1)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the AoI\n",
    "geometry.explore(\n",
    "    tiles=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n",
    "    attr=\"Esri\",\n",
    "    name=\"Esri Satellite\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "### Create spatiotemporal query using a STAC API as the backend\n",
    "This establishes the spatial and temporal extent used to search for Landsat satellite data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data, mask it and generate composites.\n",
    "# This is lazy-loaded, so no data is actually loaded yet.\n",
    "ds, items = load_and_mask_data_with_stac(config, query, include_nir=True, debug=True)\n",
    "\n",
    "print(f\"Found {len(items)} items\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Remove scenes that are completely at an extreme high or low tide\n",
    "\n",
    "For each satellite timestep, model tide heights into a low-resolution 5 x 5 km grid (matching resolution of the FES2014 tidal model), then use that to filter out scenes with all high or low tides (those outside the middle 50% of tides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered = filter_by_tides(ds, tide_data_location, tide_centre)\n",
    "\n",
    "print(f\"Reduced from {len(ds.time)} to {len(filtered.time)} timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data into memory\n",
    "\n",
    "This step loads daily cloud masked data into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optionally load the daily dataset into memory, either do this here or\n",
    "# down below for the combined dataset. This takes more memory, but is\n",
    "# faster. The below one results in a big, complex dask graph, but saves\n",
    "# a fair bit of memory.\n",
    "filtered = filtered.compute()\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tidal masking\n",
    "\n",
    "Here we do a per-pixel mask of the extreme tide pixels. This is done with data loaded into memory, to keep things efficient on memory and dask graphs.\n",
    "\n",
    "The graph below shows the percentage of data remaining each year after per-pixel masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Per-pixel tide masking\n",
    "pixel_tide_masked = mask_pixels_by_tide(filtered, tide_data_location, tide_centre)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the percentage of pixels remaining after high-res tide masking\n",
    "filter_null_byyear = filtered.mndwi.isnull().groupby(\"time.year\").sum(dim=[\"time\", \"x\", \"y\"])\n",
    "masked_null_byyear = pixel_tide_masked.mndwi.isnull().groupby(\"time.year\").sum(dim=[\"time\", \"x\", \"y\"])\n",
    "diff = (filter_null_byyear / masked_null_byyear) * 100\n",
    "diff.plot.line(x=\"year\", ylim=[0, 100], figsize=(12, 4), yticks=range(0, 101, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create annual summary datasets\n",
    "\n",
    "Here we take the daily data and combine it into one- and three-year composites, before\n",
    "gapfilling the one-year composite with the three-year composite where there are\n",
    "not enough observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a yearly dataset, loaded into memory. This takes a long time!\n",
    "combined_ds = generate_yearly_composites(pixel_tide_masked, start_year, end_year, include_nir=True, debug=True)\n",
    "\n",
    "# Load the combined dataset instead. Make sure you comment out the filtered\n",
    "# load step. This will take longer, but use less memory.\n",
    "# combined_ds = combined_ds.compute()\n",
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in [2012, 2015, 2018]:\n",
    "    for band in [\"green\", \"nir\", \"swir\", \"mndwi\"]:\n",
    "        combined_ds.sel(year=year)[band].odc.write_cog(f\"head_{year}_{band}.tif\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds.mndwi.plot.imshow(col=\"year\", col_wrap=4, cmap=\"RdBu_r\", vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_nir_land = True\n",
    "nir_threshold = 0.2\n",
    "\n",
    "thresholded_ds = combined_ds.copy(deep=True)\n",
    "# Set any pixels with only one observation to NaN, as these are\n",
    "# extremely vulnerable to noise\n",
    "masked_ds = thresholded_ds.where(combined_ds[\"count\"] > 1)\n",
    "\n",
    "# Apply water index threshold and re-apply nodata values\n",
    "nodata = masked_ds[water_index].isnull()\n",
    "ndwi_land = masked_ds[water_index] < index_threshold\n",
    "\n",
    "# Include a NIR threshold, from DE Pacific's example\n",
    "if include_nir_land:\n",
    "    nir_land = combined_ds.nir < nir_threshold\n",
    "    thresholded_ds = ndwi_land & nir_land\n",
    "else:\n",
    "    thresholded_ds = ndwi_land\n",
    "\n",
    "# Mask out the nodata\n",
    "thresholded_ds = thresholded_ds.where(~nodata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nir_land = combined_ds.nir > nir_threshold\n",
    "nir_land.plot.imshow(col=\"year\", col_wrap=4, cmap=\"Greys_r\", vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndwi_land = masked_ds[water_index] > index_threshold\n",
    "ndwi_land.plot.imshow(col=\"year\", col_wrap=4, cmap=\"Greys_r\", vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_ds.sel(year=2018).green.plot.hist(bins=100, range=[0, 0.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholded_ds = ndwi_land & nir_land\n",
    "thresholded_ds.plot.imshow(col=\"year\", col_wrap=4, cmap=\"Greys_r\", vmin=0, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nir_land.sel(year=2015).plot.imshow(cmap=\"Reds\", vmin=0, vmax=1, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndwi_land.sel(year=2015).plot.imshow(cmap=\"Greys_r\", vmin=0, vmax=1, size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_land = nir_land & ndwi_land\n",
    "both_land.sel(year=2019).plot.imshow(cmap=\"Blues\", vmin=0, vmax=2, size=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vector data and pre-process raster data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coastal mask modifications with \"add\" and \"remove\" areas\n",
    "modifications_gdf = gpd.read_file(\n",
    "    config[\"Input files\"][\"modifications_path\"], bbox=bbox\n",
    ").to_crs(str(combined_ds.odc.crs))\n",
    "\n",
    "# Mask dataset to focus on coastal zone only\n",
    "(\n",
    "    masked_ds,\n",
    "    certainty_masks,\n",
    "    all_time_20,\n",
    "    all_time_80,\n",
    "    river_mask,\n",
    "    ocean_da,\n",
    "    thresholded_ds,\n",
    "    temporal_mask,\n",
    "    annual_mask,\n",
    "    coastal_mask,\n",
    "    ocean_mask,\n",
    ") = contours_preprocess(\n",
    "    combined_ds=combined_ds,\n",
    "    water_index=water_index,\n",
    "    index_threshold=index_threshold,\n",
    "    buffer_pixels=50,\n",
    "    mask_with_esa_wc=True,\n",
    "    modifications_gdf=modifications_gdf,\n",
    "    debug=True,\n",
    "    # include_nir_land=True,\n",
    "    # nir_threshold=0.2\n",
    ")\n",
    "\n",
    "# Plot a single timestep\n",
    "masked_ds.sel(year=2017).plot.imshow(size=8, cmap=\"RdBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract shorelines\n",
    "contours = subpixel_contours(\n",
    "    da=masked_ds,\n",
    "    z_values=index_threshold,\n",
    "    min_vertices=10,\n",
    "    dim=\"year\",\n",
    ").set_index(\"year\")\n",
    "\n",
    "if len(contours) == 0:\n",
    "    raise ValueError(\"No contours found. Try a lower index threshold.\")\n",
    "\n",
    "# Add quality measures\n",
    "contours_with_certainty = contour_certainty(contours, certainty_masks)\n",
    "\n",
    "# Change the year index to a normal geopandas field\n",
    "plot_contours = contours_with_certainty.reset_index()\n",
    "\n",
    "# Plot shorelines\n",
    "plot_contours.explore(\n",
    "    tiles=\"https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}\",\n",
    "    attr=\"Esri\",\n",
    "    name=\"Esri Satellite\",\n",
    "    cmap=\"magma\",\n",
    "    column=\"year\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raise Exception(\"Pop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute statistics\n",
    "\n",
    "###  Create stats points on baseline shorline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract statistics modelling points along baseline shoreline\n",
    "try:\n",
    "    points = points_on_line(contours_with_certainty, baseline_year, distance=30)\n",
    "except KeyError:\n",
    "    print(\"Failed to make points\")\n",
    "    points_gdf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure annual coastline movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if points is not None and len(points) > 0:\n",
    "    # Calculate annual movements for every shoreline\n",
    "    # compared to the baseline year\n",
    "    points = annual_movements(\n",
    "        points,\n",
    "        contours_with_certainty,\n",
    "        combined_ds,\n",
    "        str(baseline_year),\n",
    "        water_index,\n",
    "        max_valid_dist=1200,\n",
    "    )\n",
    "    \n",
    "    # Reindex to add any missing annual columns to the dataset\n",
    "    points = points.reindex(\n",
    "        columns=[\n",
    "            \"geometry\",\n",
    "            *[f\"dist_{i}\" for i in range(start_year, end_year + 1)],\n",
    "            \"angle_mean\",\n",
    "            \"angle_std\",\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    print(\"Something went wrong! Check the points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if points is not None and len(points) > 0:\n",
    "    # Apply regression function to each row in dataset\n",
    "    points = calculate_regressions(points)\n",
    "\n",
    "    # Add count and span of valid obs, Shoreline Change Envelope (SCE),\n",
    "    # Net Shoreline Movement (NSM) and Max/Min years\n",
    "    stats_list = [\"valid_obs\", \"valid_span\", \"sce\", \"nsm\", \"max_year\", \"min_year\"]\n",
    "    points[stats_list] = points.apply(\n",
    "        lambda x: all_time_stats(x, initial_year=start_year), axis=1\n",
    "    )\n",
    "\n",
    "    # And add certainty\n",
    "    points_with_certainty = points_certainty(points, baseline_year=baseline_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "clipped_points_gdf = points_with_certainty.clip(geometry.to_crs(points_with_certainty.crs))\n",
    "clipped_contours_gdf = contours_with_certainty.clip(geometry.to_crs(contours_with_certainty.crs))\n",
    "\n",
    "out_files = export_results(clipped_points_gdf, clipped_contours_gdf, version, str(output_dir), study_area)\n",
    "\n",
    "for out_file in out_files:\n",
    "    print(f\"Exported {out_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "## Additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** For assistance with any of the Python code or Jupyter Notebooks in this repository, please post a [Github issue](https://github.com/GeoscienceAustralia/dea-coastlines/issues/new).\n",
    "\n",
    "**Last modified:** November 2022\n",
    "\n",
    "**To cite:**\n",
    "\n",
    "> Bishop-Taylor, R., Nanson, R., Sagar, S., Lymburner, L. (2021). Mapping Australia's dynamic coastline at mean sea level using three decades of Landsat imagery. Remote Sensing of Environment, 267, 112734. Available: https://doi.org/10.1016/j.rse.2021.112734\n",
    ">\n",
    "> Nanson, R., Bishop-Taylor, R., Sagar, S., Lymburner, L., (2022). Geomorphic insights into Australia's coastal change using a national dataset derived from the multi-decadal Landsat archive. Estuarine, Coastal and Shelf Science, 265, p.107712. Available: https://doi.org/10.1016/j.ecss.2021.107712\n",
    ">\n",
    "> Bishop-Taylor, R., Sagar, S., Lymburner, L., Alam, I., Sixsmith, J. (2019). Sub-pixel waterline extraction: characterising accuracy and sensitivity to indices and spectra. Remote Sensing, 11 (24):2984. Available: https://doi.org/10.3390/rs11242984"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
