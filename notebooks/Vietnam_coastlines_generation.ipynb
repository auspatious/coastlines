{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vietnam Coastlines Combined\n",
    "\n",
    "\n",
    "* Load stack of all available Landsat 5, 7, 8 and 9 satellite imagery for a location \n",
    "* Convert each satellite image into a remote sensing water index (MNDWI)\n",
    "* For each satellite image, model ocean tides into a grid based on exact time of image acquisition\n",
    "* Interpolate tide heights into spatial extent of image stack using the [FES2014 global tide model](https://github.com/GeoscienceAustralia/dea-coastlines/wiki/Setting-up-tidal-models-for-DEA-Coastlines)\n",
    "* Mask out high and low tide pixels by removing all observations acquired outside of 50 percent of the observed tidal range centered over mean sea level\n",
    "* Combine tidally-masked data into annual median composites representing the most representative position of the coastline at approximately mean sea level each year\n",
    "* Apply morphological extraction algorithms to mask annual median composite rasters to a valid coastal region\n",
    "* Extract waterline vectors using subpixel waterline extraction ([Bishop-Taylor et al. 2019b](https://doi.org/10.3390/rs11242984))\n",
    "* Compute rates of coastal change at every 30 m using linear regression\n",
    "\n",
    "This is an interactive version of the code intended for prototyping; to run this analysis at scale, use the [command line tools](DEACoastlines_generation_CLI.ipynb).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started\n",
    "\n",
    "Set working directory to top level of repository to ensure links work correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages\n",
    "\n",
    "First we import the required Python packages, then we connect to the database, and load the catalog of virtual products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"USE_PYGEOS\"] = \"0\"\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Load DEA Coastlines and DEA tools code\n",
    "import coastlines.raster\n",
    "import coastlines.utils\n",
    "import coastlines.vector\n",
    "\n",
    "from coastlines.utils import get_study_site_geometry\n",
    "from coastlines.combined import (\n",
    "    load_and_mask_data_with_stac,\n",
    "    export_results,\n",
    "    filter_by_tides,\n",
    "    generate_yearly_composites,\n",
    ")\n",
    "from coastlines.vector import contours_preprocess\n",
    "\n",
    "# Load other libraries\n",
    "from pathlib import Path\n",
    "import folium\n",
    "import geohash as gh\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datacube.utils.dask import start_local_dask\n",
    "from datacube.utils.geometry import Geometry\n",
    "from dea_tools.coastal import pixel_tides\n",
    "from dea_tools.spatial import subpixel_contours\n",
    "from odc.algo import mask_cleanup, erase_bad, to_f32\n",
    "from odc.stac import configure_s3_access, load\n",
    "from pystac_client import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Restart the kernel before re-running this!\n",
    "\n",
    "# Create local dask client for parallelisation\n",
    "dask_client = start_local_dask(\n",
    "    n_workers=8, threads_per_worker=4, mem_safety_margin=\"2GB\"\n",
    ")\n",
    "\n",
    "# Configure S3 access including request payer\n",
    "_ = configure_s3_access(requester_pays=True, cloud_defaults=True)\n",
    "\n",
    "print(dask_client.dashboard_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Study area selection\n",
    "# study_area = \"13,45\" # North\n",
    "study_area = \"9,19\"    # South west\n",
    "# study_area = \"18,32\" # Central\n",
    "\n",
    "# study_area = \"16,48\" # Empty tile\n",
    "\n",
    "# Config\n",
    "version = \"testing\"\n",
    "start_year = 2020\n",
    "end_year = 2022\n",
    "baseline_year = 2021\n",
    "water_index = \"mndwi\"\n",
    "index_threshold = 0.0\n",
    "\n",
    "config_path = \"configs/vietnam_coastlines_config_development.yaml\"\n",
    "\n",
    "# Output config\n",
    "output_dir = Path(f\"data/interim/vector/{version}/{study_area}_{version}\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Load analysis params from config file\n",
    "config = coastlines.utils.load_config(config_path=config_path)\n",
    "\n",
    "# Load the geometry from the grid used for the location\n",
    "geometry = get_study_site_geometry(config[\"Input files\"][\"grid_path\"], study_area)\n",
    "\n",
    "# BBOX and other query parameters\n",
    "boundingbox = geometry.buffer(0.05).boundingbox\n",
    "bbox = [boundingbox.left, boundingbox.bottom, boundingbox.right, boundingbox.top]\n",
    "\n",
    "# Use the USGS STAC API to identify scenes to load\n",
    "query = {\n",
    "    \"bbox\": bbox,\n",
    "    \"datetime\": (str(start_year - 1), str(end_year + 1)),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "### Create spatiotemporal query using a STAC API as the backend\n",
    "This establishes the spatial and temporal extent used to search for Landsat satellite data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load the data using dask\n",
    "ds, items = load_and_mask_data_with_stac(config, query)\n",
    "\n",
    "print(f\"Found {len(items)} items\")\n",
    "\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tidal modelling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Interpolate tides into each satellite timestep\n",
    "For each satellite timestep, model tide heights into a low-resolution 5 x 5 km grid (matching resolution of the FES2014 tidal model), then reproject modelled tides into the spatial extent of our satellite image. Add  this new data as a new variable in our satellite dataset to allow each satellite pixel to be analysed and filtered/masked based on the tide height at the exact moment of satellite image acquisition. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "filtered = filter_by_tides(ds, \"/home/jovyan/tide_models\", 0.0, use_highres=False)\n",
    "\n",
    "print(f\"Reduced from {len(ds.time)} to {len(filtered.time)} timesteps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate yearly composites in memory\n",
    "Export tidally-masked MNDWI median composites for each year, and three-yearly composites used to gapfill poor data coverage areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optionally load the daily dataset into memory, either do this here or\n",
    "# down below for the combined dataset. This takes more memory, but is\n",
    "# faster. The below one results in a big, complex dask graph, but saves\n",
    "# a fair bit of memory.\n",
    "filtered = filtered.compute()\n",
    "filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a yearly dataset, loaded into memory. This takes a long time!\n",
    "combined_ds = generate_yearly_composites(filtered, start_year, end_year)\n",
    "\n",
    "# Load the combined dataset instead. Make sure you comment out the filtered\n",
    "# load step. This will take longer, but use less memory.\n",
    "# combined_ds = combined_ds.compute()\n",
    "combined_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally dump the loaded data to disk, so that it's possible to\n",
    "# work on data cleaning load the second step without a huge load time.\n",
    "\n",
    "# from pathlib import Path\n",
    "# import shutil\n",
    "\n",
    "# out_data = Path(f\"data/interim/raster/{version}/{study_area.replace(',', '_')}_{version}_combined_ds.zarr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below to save data\n",
    "\n",
    "# if out_data.exists():\n",
    "#     print(f\"Folder {out_data} already exists. Deleting...\")\n",
    "#     shutil.rmtree(out_data)\n",
    "\n",
    "# combined_ds.to_zarr(out_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the below to load data\n",
    "# combined_ds = xr.open_zarr(out_data).load()\n",
    "# combined_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load vector data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Coastal mask modifications\n",
    "modifications_gdf = gpd.read_file(\n",
    "    config[\"Input files\"][\"modifications_path\"], bbox=bbox\n",
    ").to_crs(str(combined_ds.odc.crs))\n",
    "\n",
    "# Mask dataset to focus on coastal zone only\n",
    "\n",
    "# NOTE THAT MASK WITH ESA IS False, because it's failing currently!!\n",
    "(\n",
    "    masked_ds,\n",
    "    certainty_masks,\n",
    "    all_time_20,\n",
    "    all_time_80,\n",
    "    river_mask,\n",
    "    ocean_da,\n",
    "    thresholded_ds,\n",
    "    temporal_mask,\n",
    "    annual_mask,\n",
    "    coastal_mask,\n",
    "    ocean_mask,\n",
    ") = contours_preprocess(\n",
    "    combined_ds=combined_ds,\n",
    "    water_index=water_index,\n",
    "    index_threshold=index_threshold,\n",
    "    buffer_pixels=50,\n",
    "    mask_with_esa_wc=False,\n",
    "    mask_modifications=modifications_gdf,\n",
    "    debug=True\n",
    ")\n",
    "\n",
    "# Plot a single timestep\n",
    "masked_ds.isel(year=0).plot(size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_ds,\n",
    "# certainty_masks,\n",
    "# all_time_20,\n",
    "# all_time_80,\n",
    "# river_mask,\n",
    "# ocean_da,\n",
    "# thresholded_ds,\n",
    "# temporal_mask,\n",
    "# annual_mask,\n",
    "# coastal_mask,\n",
    "# ocean_mask,\n",
    "\n",
    "all_time_80.plot.imshow(size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract shorelines\n",
    "contours_gdf = subpixel_contours(\n",
    "    da=masked_ds,\n",
    "    z_values=index_threshold,\n",
    "    min_vertices=10,\n",
    "    dim=\"year\",\n",
    ").set_index(\"year\")\n",
    "\n",
    "# Plot shorelines\n",
    "contours_gdf.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preview the data on a web map\n",
    "bb = masked_ds.odc.geobox.boundingbox.to_crs(4326)\n",
    "d_x = bb.right - bb.left\n",
    "d_y = bb.top - bb.bottom\n",
    "\n",
    "location = (bb.bottom + d_y / 2, bb.left + d_x / 2) \n",
    "\n",
    "map = folium.Map(\n",
    "  location=location,\n",
    "  tiles='https://server.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n",
    "  attr='Esri',\n",
    "  name='Esri Satellite',\n",
    "  zoom_start=10\n",
    ")\n",
    "\n",
    "for _, y in contours_gdf.to_crs(\"epsg:4326\").iterrows():\n",
    "    for r in y:\n",
    "      sim_geo = gpd.GeoSeries(r.geoms)\n",
    "      geo_j = sim_geo.to_json()\n",
    "      geo_j = folium.GeoJson(data=geo_j, style_function=lambda x: {\"stroke\": \"red\"})\n",
    "      geo_j.add_to(map)\n",
    "map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute statistics\n",
    "\n",
    "###  Create stats points on baseline shorline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract statistics modelling points along baseline shoreline\n",
    "try:\n",
    "    points_gdf = coastlines.vector.points_on_line(contours_gdf, baseline_year, distance=30)\n",
    "except KeyError:\n",
    "    print(\"Failed to make points\")\n",
    "    points_gdf = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Measure annual coastline movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if points_gdf is not None and len(points_gdf) > 0:\n",
    "    \n",
    "    # Calculate annual movements for every shoreline\n",
    "    # compared to the baseline year\n",
    "    points_gdf = coastlines.vector.annual_movements(\n",
    "        points_gdf,\n",
    "        contours_gdf,\n",
    "        combined_ds,\n",
    "        str(baseline_year),\n",
    "        water_index,\n",
    "        max_valid_dist=1200,\n",
    "    )\n",
    "    \n",
    "    # Reindex to add any missing annual columns to the dataset\n",
    "    points_gdf = points_gdf.reindex(\n",
    "        columns=[\n",
    "            \"geometry\",\n",
    "            *[f\"dist_{i}\" for i in range(start_year, end_year + 1)],\n",
    "            \"angle_mean\",\n",
    "            \"angle_std\",\n",
    "        ]\n",
    "    )\n",
    "else:\n",
    "    print(\"Something went wrong! Check the points.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if points_gdf is not None and len(points_gdf) > 0:\n",
    "    # Apply regression function to each row in dataset\n",
    "    points_gdf = coastlines.vector.calculate_regressions(points_gdf)\n",
    "\n",
    "    # Add count and span of valid obs, Shoreline Change Envelope (SCE),\n",
    "    # Net Shoreline Movement (NSM) and Max/Min years\n",
    "    stats_list = [\"valid_obs\", \"valid_span\", \"sce\", \"nsm\", \"max_year\", \"min_year\"]\n",
    "    points_gdf[stats_list] = points_gdf.apply(\n",
    "        lambda x: coastlines.vector.all_time_stats(x, initial_year=start_year), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contours_gdf.to_file(\"9_19.gpkg\", format=\"GPKG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "export_results(points_gdf, contours_gdf, version, output_dir, study_area)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dask_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "\n",
    "## Additional information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**License:** The code in this notebook is licensed under the [Apache License, Version 2.0](https://www.apache.org/licenses/LICENSE-2.0). \n",
    "Digital Earth Australia data is licensed under the [Creative Commons by Attribution 4.0](https://creativecommons.org/licenses/by/4.0/) license.\n",
    "\n",
    "**Contact:** For assistance with any of the Python code or Jupyter Notebooks in this repository, please post a [Github issue](https://github.com/GeoscienceAustralia/dea-coastlines/issues/new).\n",
    "\n",
    "**Last modified:** November 2022\n",
    "\n",
    "**To cite:**\n",
    "\n",
    "> Bishop-Taylor, R., Nanson, R., Sagar, S., Lymburner, L. (2021). Mapping Australia's dynamic coastline at mean sea level using three decades of Landsat imagery. Remote Sensing of Environment, 267, 112734. Available: https://doi.org/10.1016/j.rse.2021.112734\n",
    ">\n",
    "> Nanson, R., Bishop-Taylor, R., Sagar, S., Lymburner, L., (2022). Geomorphic insights into Australia's coastal change using a national dataset derived from the multi-decadal Landsat archive. Estuarine, Coastal and Shelf Science, 265, p.107712. Available: https://doi.org/10.1016/j.ecss.2021.107712\n",
    ">\n",
    "> Bishop-Taylor, R., Sagar, S., Lymburner, L., Alam, I., Sixsmith, J. (2019). Sub-pixel waterline extraction: characterising accuracy and sensitivity to indices and spectra. Remote Sensing, 11 (24):2984. Available: https://doi.org/10.3390/rs11242984"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
